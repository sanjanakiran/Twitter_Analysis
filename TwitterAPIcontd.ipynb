{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS600 - Social Media & Data Mining\n",
    "###  \n",
    "<img src=\"https://www.syracuse.edu/wp-content/themes/g6-carbon/img/syracuse-university-seal.svg?ver=6.3.9\" style=\"width: 200px;\"/>\n",
    "\n",
    "# Twitter\n",
    "\n",
    "###  February 15, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twitter import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading my authentication tokens\n",
    "with open('auth_dict','r') as f:\n",
    "    twtr_auth = json.load(f)\n",
    "\n",
    "# To make it more readable, lets store\n",
    "# the OAuth credentials in strings first.\n",
    "CONSUMER_KEY = twtr_auth['consumer_key']\n",
    "CONSUMER_SECRET = twtr_auth['consumer_secret']\n",
    "OAUTH_TOKEN = twtr_auth['token']\n",
    "OAUTH_TOKEN_SECRET = twtr_auth['token_secret']\n",
    "    \n",
    "# Then, we store the OAuth object in \"auth\"\n",
    "auth = OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
    "# Notice that there are four tokens - you need to create these in the\n",
    "# Twitter Apps dashboard after you have created your own \"app\".\n",
    "\n",
    "# We now create the twitter search object.\n",
    "t = Twitter(auth=auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can search trends by geographic region. The Yahoo! Where On Earth ID for the entire world is 1, for example. Look [here](https://developer.twitter.com/en/docs/trends/trends-for-location/api-reference/get-trends-place) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORLD_WOE_ID = 1\n",
    "US_WOE_ID = 23424977"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get the trends (top 50) for the whole world, and then for the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prefix ID with the underscore for query string parameterization.\n",
    "# Without the underscore, the twitter package appends the ID value\n",
    "# to the URL itself as a special case keyword argument.\n",
    "\n",
    "world_trends = t.trends.place(_id=WORLD_WOE_ID)\n",
    "us_trends = t.trends.place(_id=US_WOE_ID)\n",
    "\n",
    "print(world_trends)\n",
    "print('\\n')\n",
    "print(us_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That doesn't look so good, so now we \"pretty print\" it using the JSON module. The builtin *print* gives better output when its input is a JSON object (rather than, say, a string or list object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(json.dumps(world_trends, indent=1))\n",
    "print('\\n')\n",
    "print(json.dumps(us_trends, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have both results in memory, we can combine them. For example, we find the intersection of the top world trends and the top USA trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Why bother with \"set\" here?\n",
    "world_trends_set = set([trend['name'] for trend in world_trends[0]['trends']])\n",
    "\n",
    "us_trends_set = set([trend['name'] for trend in us_trends[0]['trends']]) \n",
    "\n",
    "common_trends = world_trends_set.intersection(us_trends_set)\n",
    "\n",
    "print(common_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, let's collect search results in a loop. Note that we are already able to do this with the Search API and that the Streaming API will be needed for larger volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The *count* parameter below is the same parameter we saw before and represents the number of tweets we want *per page*. This was stated incorrectly in Tuesday's lecture and notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is important because the Twitter REST API uses *cursoring* to organize large search results in *pages*. The next example shows how to use the cursor. Read more about it [here](https://developer.twitter.com/en/docs/basics/cursoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the next variable to any trending topic\n",
    "# or anything else for that matter.\n",
    "q = '\"Super Bowl\"' \n",
    "\n",
    "# Same as before - the number of tweets we want *per page*.\n",
    "count = 1000\n",
    "\n",
    "# We do the search call.\n",
    "search_results = t.search.tweets(q=q, count=count)\n",
    "\n",
    "# Remember 'status' refers to the actual content of a tweet.\n",
    "# (as opposed to the metadata)\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "\n",
    "# Iterate through 5 more batches of results by following the cursor.\n",
    "# The use of the underscore \"_\" below is a convention in python \n",
    "# indicating to the reader that the variable will not be used for\n",
    "# anything within the loop.\n",
    "for _ in range(5):\n",
    "    print(\"Length of statuses \", len(statuses))\n",
    "    # Remember \"try...except\"? Here it is in action:\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    # No more results when next_results doesn't exist\n",
    "    except KeyError as e: \n",
    "        break\n",
    "        \n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in next_results[1:].split(\"&\") ])\n",
    "    \n",
    "    search_results = t.search.tweets(**kwargs)\n",
    "    statuses += search_results['statuses']\n",
    "\n",
    "# Show one sample search result by slicing the list...\n",
    "print(json.dumps(statuses[0], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note the \"except\" line in the cell above. Including \"as e\" after the \"KeyError\" lets you access the attributes of the error - we didn't do that, but you might find a need for it. Inside the indent, you could manipulate the object e and get its attributes, such as e.args.\n",
    "\n",
    "### Instead, we simply break out of the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the next example, we extract text, screen names and hashtags from tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "status_texts = [ status['text'] for status in statuses ]\n",
    "\n",
    "screen_names = [ user_mention['screen_name'] for status in statuses\n",
    "                     for user_mention in status['entities']['user_mentions'] ]\n",
    "\n",
    "hashtags = [ hashtag['text'] for status in statuses\n",
    "                 for hashtag in status['entities']['hashtags'] ]\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [ w for t in status_texts for w in t.split() ]\n",
    "\n",
    "# Explore the first 5 items for each...\n",
    "\n",
    "print(json.dumps(status_texts[0:5], indent=1))\n",
    "print(json.dumps(screen_names[0:5], indent=1))\n",
    "print(json.dumps(hashtags[0:5], indent=1))\n",
    "print(json.dumps(words[0:5], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can create a basic frequency distribution from words in tweets. A first glimpse at NLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Recall that you can loop through any\n",
    "# iterator in python - such as a list\n",
    "# of lists!\n",
    "for item in [words, screen_names, hashtags]:\n",
    "    c = Counter(item)\n",
    "    print(c.most_common()[:10]) # top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going a little further, we can calculate the *lexical diversity* of our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function for computing lexical diversity\n",
    "def lexical_diversity(tokens):\n",
    "    return 1.0*len(set(tokens))/len(tokens) \n",
    "\n",
    "# A function for computing the average number of words per tweet\n",
    "def average_words(statuses):\n",
    "    total_words = sum([ len(s.split()) for s in statuses ]) \n",
    "    return total_words/len(statuses)\n",
    "\n",
    "print(lexical_diversity(words))\n",
    "print(lexical_diversity(screen_names))\n",
    "print(lexical_diversity(hashtags))\n",
    "print(average_words(status_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: why didn't we multiply by \"1.0\" in the return value of \"average_words\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can plot the frequencies of words using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Word Rank')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First, we get the frequencies in order.\n",
    "word_counts = sorted(Counter(words).values(), reverse=True)\n",
    "\n",
    "# We can plot it along log-scaled axes\n",
    "plt.loglog(word_counts)\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.xlabel(\"Word Rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now we \"show\" it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also do histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for label, data in (('Words', words), \n",
    "                    ('Screen Names', screen_names), \n",
    "                    ('Hashtags', hashtags)):\n",
    "\n",
    "    # Build a frequency map for each set of data\n",
    "    # and plot the values\n",
    "    c = Counter(data)\n",
    "    plt.hist(c.values())\n",
    "    \n",
    "    # Add a title and y-label ...\n",
    "    plt.title(label)\n",
    "    plt.ylabel(\"Number of items in bin\")\n",
    "    plt.xlabel(\"Bins (number of times an item appeared)\")\n",
    "    \n",
    "    # ... and display as a new figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now for something completely different...finding the most popular retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "retweets = [\n",
    "            # We are building a list of tuples\n",
    "            (status['retweet_count'], \n",
    "             status['retweeted_status']['user']['screen_name'],\n",
    "             status['text'],\n",
    "             status['retweeted_status']) \n",
    "            \n",
    "            # ... for each status ...\n",
    "            for status in statuses \n",
    "            \n",
    "            # ... so long as the status meets this condition.\n",
    "                if 'retweeted_status' in status\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rmk: In Python2, dictionaries have a \"has_key\" method. This was dropped from Python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking up users who have retweeted a 'status'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's pick one of the retweets. We will find the id of the original tweet from the 'retweeted_status' node (the last element of our tuple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "963844963975954432"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweets[0][-1]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the id of the retweeted tweet. We will use this to search Twitter again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that we are doing another search here.\n",
    "_retweets = t.statuses.retweets(id=963844963975954432)\n",
    "print([r['user']['screen_name'] for r in _retweets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's look at a histogram of retweet counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = [count for count, _, _, _ in retweets]\n",
    "\n",
    "plt.hist(counts)\n",
    "plt.title(\"Retweets\")\n",
    "plt.xlabel('Bins (number of times retweeted)')\n",
    "plt.ylabel('Number of tweets in bin')\n",
    "\n",
    "print(counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to reuse the code from the cells above. Also, we want to write clean and readable code when we implement longer and more complex procedures. So let's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oauth_login():\n",
    "    # Replace credentials below with appropriate values - \n",
    "    # this works in the TwitterAPIcontd notebook because\n",
    "    # we defined the credentials above\n",
    "    CONSUMER_KEY = CONSUMER_KEY\n",
    "    CONSUMER_SECRET = CONSUMER_SECRET\n",
    "    OAUTH_TOKEN = OAUTH_TOKEN\n",
    "    OAUTH_TOKEN_SECRET = OAUTH_TOKEN_SECRET\n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "    CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we can wrap the lines that give us trends in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twitter_trends(twitter_api, woe_id):\n",
    "    # Prefix ID with the underscore for query string parameterization.\n",
    "    # Without the underscore, the twitter package appends the ID value\n",
    "    # to the URL itself as a special-case keyword argument.\n",
    "    return twitter_api.trends.place(_id=woe_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likewise, we define a function for the looped twitter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twitter_search(twitter_api, q, max_results=200, **kw):\n",
    "    search_results = twitter_api.search.tweets(q=q, count=100, **kw)\n",
    "    statuses = search_results['statuses']\n",
    "    # Enforce a reasonable limit\n",
    "    max_results = min(1000, max_results)\n",
    "    for _ in range(10): # 10*100 = 1000\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError as e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') \n",
    "                       for kv in next_results[1:].split(\"&\") ])\n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        if len(statuses) > max_results:\n",
    "            break\n",
    "    \n",
    "    return statuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You will likely encounter errors in mining Twitter data. Here is a function to automate the handling of certain errors. See *Mining the Social Web* for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from twitter.api import TwitterHTTPError\n",
    "from urllib.error import URLError\n",
    "from http.client import BadStatusLine\n",
    "\n",
    "def make_twitter_request(twitter_api_func, max_errors=10, *args, **kw):\n",
    "    # A nested helper function that handles common HTTPErrors. Return an updated\n",
    "    # value for wait_period if the problem is a 500 level error. Block until the\n",
    "    # rate limit is reset if it's a rate limiting issue (429 error). Returns None\n",
    "    # for 401 and 404 errors, which requires special handling by the caller.\n",
    "    def handle_twitter_http_error(e, wait_period=2, sleep_when_rate_limited=True):\n",
    "        if wait_period > 3600: # Seconds\n",
    "            print('Too many retries. Quitting.', file=sys.stderr)\n",
    "            raise e\n",
    "        if e.e.code == 401:\n",
    "            return None\n",
    "        elif e.e.code == 404:\n",
    "            print('Encountered 404 Error (Not Found)', file=sys.stderr)\n",
    "            return None\n",
    "        elif e.e.code == 429:\n",
    "            print('Encountered 429 Error (Rate Limit Exceeded)', file=sys.stderr)\n",
    "            if sleep_when_rate_limited:\n",
    "                print(\"Retrying in 15 minutes...ZzZ...\", file=sys.stderr)\n",
    "                sys.stderr.flush()\n",
    "                time.sleep(60*15 + 5)\n",
    "                print('...ZzZ...Awake now and trying again.', file=sys.stderr)\n",
    "                return 2\n",
    "            else:\n",
    "                raise e # Caller must handle the rate limiting issue\n",
    "        elif e.e.code in (500, 502, 503, 504):\n",
    "            print('Encountered %i Error. Retrying in %i seconds' % (e.e.code, wait_period), file=sys.stderr)\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            return wait_period\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # End of nested helper function\n",
    "\n",
    "    wait_period = 2\n",
    "    error_count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            return twitter_api_func(*args, **kw)\n",
    "        except TwitterHTTPError as e:\n",
    "            error_count = 0\n",
    "            wait_period = handle_twitter_http_error(e, wait_period)\n",
    "            if wait_period is None:\n",
    "                return\n",
    "        except URLError as e:\n",
    "            error_count += 1\n",
    "            print(\"URLError encountered. Continuing.\", file=sys.stderr)\n",
    "            if error_count > max_errors:\n",
    "                print(\"Too many consecutive errors...bailing out.\", file=sys.stderr)\n",
    "                raise\n",
    "        except BadStatusLine as e:\n",
    "            error_count += 1\n",
    "            print >> sys.stderr, \"BadStatusLine encountered. Continuing.\"\n",
    "            if error_count > max_errors:\n",
    "                print(\"Too many consecutive errors...bailing out.\", file=sys.stderr)\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = make_twitter_request(t.users.lookup, screen_name=\"SocialWebMining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Streaming Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### We will want to write responses to disk, on the fly, so that we can collect many observations for later analysis. In *Mining the Social Web*, the Mongo DB database program is recommended. Here is another way (that also can be adapted so that it writes to a database)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's wrap the extraction of tweet \"[entities](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/entities-object)\" in a function which takes a list of statuses as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_tweet_entities(statuses):\n",
    "    if len(statuses) == 0:\n",
    "        return [], [], [], [], []\n",
    "    screen_names = [ user_mention['screen_name'] \n",
    "                    for status in statuses\n",
    "                        for user_mention in status['entities']['user_mentions'] ]\n",
    "    hashtags = [ hashtag['text']\n",
    "                    for status in statuses\n",
    "                        for hashtag in status['entities']['hashtags'] ]\n",
    "    urls = [ url['expanded_url']\n",
    "                    for status in statuses\n",
    "                        for url in status['entities']['urls'] ]\n",
    "    symbols = [ symbol['text']\n",
    "                    for status in statuses\n",
    "                        for symbol in status['entities']['symbols'] ]\n",
    "    if status['entities'].has_key('media'):\n",
    "        media = [ media['url']\n",
    "            for status in statuses\n",
    "                for media in status['entities']['media'] ]\n",
    "    else:\n",
    "        media = []\n",
    "    return screen_names, hashtags, urls, media, symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above function is a good template of sorts. You can use it to develop your own function for extracting other information from a *list of statuses*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's define two more functions, then we'll sample from the \"Twitter Firehose\", writing results as we go. The functions below will take statuses, extract some information, and then write the results into a CSV file using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_tweet_basics(status):\n",
    "    screen_name = None\n",
    "    tweet_ID = None\n",
    "    text = None\n",
    "    if 'user' in status:\n",
    "        screen_name = status['user']['screen_name'] \n",
    "        tweet_ID = status['id']\n",
    "        text = status['text']\n",
    "    return screen_name, tweet_ID, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_to_csv(file, status):\n",
    "    screen_name, tweet_ID, text = extract_tweet_basics(status)\n",
    "    df = pd.DataFrame([[screen_name,tweet_ID,text]], columns=['screen_name','tweet_ID','text'])\n",
    "    with open(file, 'a') as f:\n",
    "        df.to_csv(f,header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we stick this in the streaming loop from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a *streaming* connection (not RESTful, different from Search).\n",
    "t_stream = TwitterStream(auth=auth)\n",
    "\n",
    "\n",
    "# Get an *iterator* object from the twitter wrapper\n",
    "\n",
    "tweeterator = t_stream.statuses.sample()\n",
    "\n",
    "# Create a CSV file with column names\n",
    "# but no data (yet).\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['screen_name','tweet_ID','text'])\n",
    "df.to_csv('my_csv.csv', index=False)\n",
    "\n",
    "\n",
    "# The loop below will grab a new tweet,\n",
    "# extract some basic info, put that info\n",
    "# in a dataframe object, then use that\n",
    "# dataframe object to append one row to\n",
    "# the existing CSV file, 'my_csv.csv'.\n",
    "\n",
    "tweet_count = 10\n",
    "for tweet in tweeterator:\n",
    "    tweet_count -= 1\n",
    "    tweet_to_csv('my_csv.csv', tweet)  \n",
    "    if tweet_count <= 0:\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rmk: Pandas has a *lot* going on. For example, it makes connecting to a SQL database very easy. Recording results to a database using Pandas is almost as easy as writing to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
