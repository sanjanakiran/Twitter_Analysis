{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS600 - Social Media & Data Mining\n",
    "###  \n",
    "<img src=\"https://www.syracuse.edu/wp-content/themes/g6-carbon/img/syracuse-university-seal.svg?ver=6.3.9\" style=\"width: 200px;\"/>\n",
    "\n",
    "# NLP Cont'd\n",
    "\n",
    "###  March 27, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do what we did last week using data from a different source. You can find the below files in the BB content folder for last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "# We'll need numpy, as usual\n",
    "import numpy as np\n",
    "\n",
    "# Import natural language toolkit\n",
    "import nltk\n",
    "\n",
    "# Read training data from a text file\n",
    "with open('training.txt','r') as f:\n",
    "    training_lines = f.readlines()\n",
    "\n",
    "# Read test data from a text file\n",
    "with open('testdata.txt','r') as f:\n",
    "    test_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are we working with here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_lines),len(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the size\n",
    "import sys\n",
    "sys.getsizeof(training_lines),sys.getsizeof(test_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look some examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_lines[0],training_lines[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_lines[0],test_lines[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nothing is stopping us from looking at this in a text editor or word processor, but let's poke around some more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How long are these lines, statistically speaking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating lengths\n",
    "trn_lens, test_lens = [len(x) for x in training_lines], [len(x) for x in test_lines]\n",
    "\n",
    "# Summary stats\n",
    "print((np.mean(trn_lens),np.std(trn_lens)),(np.mean(test_lens),np.std(test_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks like the test samples are a bit shorter, but also with greater variance. (Aside: how could you find whether that difference is 'real'?). We can visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First plot\n",
    "p1 = figure(title=\"Training Lengths\",\n",
    "            background_fill_color=\"#E8DDCB\", x_range=(0,300))\n",
    "hist, edges = np.histogram(trn_lens, density=True, bins=50) # Get histogram stuff from numpy\n",
    "p1.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],\n",
    "        fill_color=\"#036564\", line_color=\"#033649\") # Add quadrilaterals to the figure\n",
    "\n",
    "# Second plot\n",
    "p2 = figure(title=\"Test Lengths\",\n",
    "            background_fill_color=\"#E8DDCB\", x_range=(0,300))\n",
    "hist, edges = np.histogram(test_lens, density=True, bins=200) # Get histogram stuff from numpy\n",
    "p2.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],\n",
    "        fill_color=\"#036564\", line_color=\"#033649\") # Add quadrilaterals to the figure\n",
    "\n",
    "# Combine plots and show\n",
    "output_notebook()\n",
    "show(gridplot(p1,p2,ncols=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It looks like the lines of `training_lines` have `0` or `1` prepended - is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all([x[0] == '0' or x[0] == '1' for x in training_lines])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But not the test lines? What good is that? This data is from a [*Kaggle competition*](https://inclass.kaggle.com/c/si650winter11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We won't bother with the test data. But let's process the data in *training.txt* as we did the corpus data. The first step is to collect all the words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(x.split()[1:],x[0]) for x in training_lines]\n",
    "for x,y in documents[:5]:\n",
    "    print('\"' + ' '.join(x) + '\"' + ' is in category ' + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall that we had to shuffle the documents before splitting them up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we want to filter the short words out of our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = []\n",
    "\n",
    "for (words,sentiment) in documents:\n",
    "    words_filtered = [w.lower() for w in words if len(w) >= 3]\n",
    "    filtered.append((words_filtered,sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great, what does this look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks like we could still clean it up a little more, for example taking out the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking out (some of) the punctuation\n",
    "# - why not put it back into 'documents'?\n",
    "\n",
    "documents = []\n",
    "\n",
    "for (words, sent) in filtered:\n",
    "    words_stripped = [w.strip('.,;!:-') for w in words]\n",
    "    documents.append((words_stripped,sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's a lot cleaner. Now on to the features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the words\n",
    "def get_words_in_docs(docs):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in docs:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "# Extract the most frequent words\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = [w for (w, c) in wordlist.most_common(1000)]\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And how are we to apply these functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of the first is the input to the second\n",
    "word_features = get_word_features(get_words_in_docs(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_features[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On to the dictionaries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unchanged from last time\n",
    "def extract_features(document): \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Encoding each document\n",
    "train_set = [(extract_features(d), c) for (d,c) in documents[:4000]]\n",
    "test_set = [(extract_features(d), c) for (d,c) in documents[4000:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = nltk.classify.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK, that's enough of that routine. You know enough python at this point to use any reasonable data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( clf.classify(test_set[0][0]), '\\n',\n",
    "      training_lines[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( clf.classify(test_set[1][0]), '\\n',\n",
    "      training_lines[4001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_lines[4000:4010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( clf.classify(test_set[3][0]), '\\n',\n",
    "      training_lines[4003])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Aside: we didn't specify the target or categories at any point. The `nltk` implementations read it off the form of our data, which is why we've prepared the data in this particular way. Bear that in mind if you're using another package for classification.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Features & Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use the `nltk` corpora to look at some other NLP concepts. Our approach thus far has been to treat each document as a *bag of words*. That lets a lot of meaning slip through (in different ways), and there is much more useable information we can pull out of text documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a badly named but important quantity. Our models so far have been so stupid that we have not seen the need for this. \n",
    "\n",
    "### In particular, we used a *one-hot* encoding - true or false - of words frequent in the corpora. What if we were to use not just those boolean values, but the frequency of certain words? \n",
    "\n",
    "### That's not a bad idea, but many meaningless words have high frequency. \n",
    "\n",
    "### Therefore, *what about words that are frequent within certain documents, but not so frequent in the whole corpus*? \n",
    "\n",
    "### Those should tell us something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's define the *term frequency* $TF(t,d)$, where $t$ is a word (loosely) and $d$ is a document:\n",
    "\n",
    "## $TF(t,d) = \\frac{\\text{occurrences of }t\\text{ in }d}{\\text{number of words in }d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, let's define the *inverse document frequency* $IDF(t)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $IDF(t) = \\log\\big(\\frac{\\text{size of corpus}}{\\text{number of documents with }t}\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the $TF$-$IDF$ is the *product* of these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing nltk stuff\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Our documents, in convenient form\n",
    "documents = [list(movie_reviews.words(fileid))\n",
    "                for category in movie_reviews.categories()\n",
    "                    for fileid in movie_reviews.fileids(category)]\n",
    "# Total documents\n",
    "N = len(documents)\n",
    "\n",
    "# Doing idf first\n",
    "def idf(t):\n",
    "    # t is a string\n",
    "    \n",
    "    # How many documents contain t?\n",
    "    docs = [d for d in documents if t in d]\n",
    "    frac = N / ( 1+ len(docs))\n",
    "    return np.log(frac)\n",
    "\n",
    "\n",
    "def tfidf(t,d):\n",
    "    # t is a string\n",
    "    # d is an int (index for \"documents\")\n",
    "    \n",
    "    # Calculating term frequency is straightforward\n",
    "    tf = documents[d].count(t) / len(documents[d])\n",
    "    \n",
    "    # Multiply, calling the other function\n",
    "    prod = tf*idf(t)\n",
    "    return prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[13].count('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.FreqDist(documents[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf('khan',13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In document retrieval, you specify query terms, or otherwise supply information to be matched in a corpus, and appropriate documents are ranked and returned. This is closely related to classification. We can use TF-IDF to build what's sometimes called the *vector space model*. Here is a 2D example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's say we want to retrieve documents pertaining to *car insurance*. That's two terms, *car* and *insurance*. Define a *query vector* $q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.array([.71,.71])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Why these values?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose we have three documents $d_1,d_2,d_3$. Suppose that for each of the two terms *car* and *insurance*, we have computed the TF-IDF for these three documents. By abuse of notation, let's use the document names for their representing vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1,d2,d3 = np.array([[.13,.99],[.8,.6],[.99,.13]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (How about these values?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm([d1,d2,d3], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK, let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"9b961304-2e98-4834-b2af-23ce0bcedf6f\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"9b961304-2e98-4834-b2af-23ce0bcedf6f\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"9b961304-2e98-4834-b2af-23ce0bcedf6f\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '9b961304-2e98-4834-b2af-23ce0bcedf6f' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.14.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"9b961304-2e98-4834-b2af-23ce0bcedf6f\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"9b961304-2e98-4834-b2af-23ce0bcedf6f\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"9b961304-2e98-4834-b2af-23ce0bcedf6f\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '9b961304-2e98-4834-b2af-23ce0bcedf6f' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.14.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"9b961304-2e98-4834-b2af-23ce0bcedf6f\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"20334130-8f06-4509-a281-e6c811ac8470\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"703118b8-5fe3-4d3d-b639-775c324461cb\":{\"roots\":{\"references\":[{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"56e68770-598d-4f99-993f-c776ad9b037a\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"c1452993-ee76-4d64-b59e-2233d3928588\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"eafdd580-981d-440e-97a7-8b84b8b25cd2\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"a8a7daa1-8510-4a0a-bb7a-018ecec9b690\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"124cb290-9d48-485c-987f-1bf364c0134a\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"c1452993-ee76-4d64-b59e-2233d3928588\",\"type\":\"PanTool\"},{\"id\":\"124cb290-9d48-485c-987f-1bf364c0134a\",\"type\":\"WheelZoomTool\"},{\"id\":\"6e8be94e-a8c4-4e6d-895c-d4329b4b0d77\",\"type\":\"BoxZoomTool\"},{\"id\":\"f61e445e-e818-4681-bfc1-e8de157252b5\",\"type\":\"SaveTool\"},{\"id\":\"fb3c5b32-8000-4fc4-8ef1-a1545b7770f4\",\"type\":\"ResetTool\"},{\"id\":\"c05cce68-682e-4a62-82b5-962b51c81717\",\"type\":\"HelpTool\"}]},\"id\":\"f96f4417-5ae0-45fa-8d1e-3fd9ef556db1\",\"type\":\"Toolbar\"},{\"attributes\":{\"below\":[{\"id\":\"4e080feb-4a83-4e9b-b834-a41caecc561a\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"e3c0efb0-cfd2-424f-b12d-ac5122ef8bcf\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"plot_width\":400,\"renderers\":[{\"id\":\"4e080feb-4a83-4e9b-b834-a41caecc561a\",\"type\":\"LinearAxis\"},{\"id\":\"a31d06ce-0e54-40b8-9725-72504a095e22\",\"type\":\"Grid\"},{\"id\":\"e3c0efb0-cfd2-424f-b12d-ac5122ef8bcf\",\"type\":\"LinearAxis\"},{\"id\":\"65014291-4b1b-4f2c-bfb5-92493a06eee0\",\"type\":\"Grid\"},{\"id\":\"56e68770-598d-4f99-993f-c776ad9b037a\",\"type\":\"BoxAnnotation\"},{\"id\":\"6c79adff-b3c0-4e23-8221-a6dbc0dae34a\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"a8a7daa1-8510-4a0a-bb7a-018ecec9b690\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"f96f4417-5ae0-45fa-8d1e-3fd9ef556db1\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1950ffd7-b8a8-4588-94eb-b0b2f2cf7282\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"71f6a3ea-f1de-4cc7-851b-b931b44b3f1f\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"568262e1-a191-4572-92b5-0a94f2c7e0ed\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"c246224a-9da6-482d-9b6d-42f7b9aefa9b\",\"type\":\"LinearScale\"}},\"id\":\"ce0c0859-24b9-4398-8931-ff3b8e71a516\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"overlay\":{\"id\":\"56e68770-598d-4f99-993f-c776ad9b037a\",\"type\":\"BoxAnnotation\"}},\"id\":\"6e8be94e-a8c4-4e6d-895c-d4329b4b0d77\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"callback\":null},\"id\":\"1950ffd7-b8a8-4588-94eb-b0b2f2cf7282\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"f61e445e-e818-4681-bfc1-e8de157252b5\",\"type\":\"SaveTool\"},{\"attributes\":{\"data_source\":{\"id\":\"9c26413c-8964-4f6e-a9b1-3423d1422b04\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"ced651a5-c802-4e20-8b2b-5909d9a97640\",\"type\":\"Segment\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"71f096e9-90aa-4dd2-b752-319f57284ce4\",\"type\":\"Segment\"},\"selection_glyph\":null,\"view\":{\"id\":\"16921fa0-c7d6-4f53-8eff-0ae2c7a5e90a\",\"type\":\"CDSView\"}},\"id\":\"6c79adff-b3c0-4e23-8221-a6dbc0dae34a\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"fb3c5b32-8000-4fc4-8ef1-a1545b7770f4\",\"type\":\"ResetTool\"},{\"attributes\":{\"callback\":null},\"id\":\"568262e1-a191-4572-92b5-0a94f2c7e0ed\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"c05cce68-682e-4a62-82b5-962b51c81717\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"71f6a3ea-f1de-4cc7-851b-b931b44b3f1f\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"c246224a-9da6-482d-9b6d-42f7b9aefa9b\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"62ea0f97-96f9-4429-9ce9-394cbbedea08\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"bfc150d8-274f-4e7e-8460-1c48f0d20988\",\"type\":\"BasicTicker\"},{\"attributes\":{\"formatter\":{\"id\":\"eafdd580-981d-440e-97a7-8b84b8b25cd2\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"ce0c0859-24b9-4398-8931-ff3b8e71a516\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"01264c65-1fc1-4c3d-bcd1-d68a67694653\",\"type\":\"BasicTicker\"}},\"id\":\"4e080feb-4a83-4e9b-b834-a41caecc561a\",\"type\":\"LinearAxis\"},{\"attributes\":{\"plot\":{\"id\":\"ce0c0859-24b9-4398-8931-ff3b8e71a516\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"01264c65-1fc1-4c3d-bcd1-d68a67694653\",\"type\":\"BasicTicker\"}},\"id\":\"a31d06ce-0e54-40b8-9725-72504a095e22\",\"type\":\"Grid\"},{\"attributes\":{\"formatter\":{\"id\":\"62ea0f97-96f9-4429-9ce9-394cbbedea08\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"ce0c0859-24b9-4398-8931-ff3b8e71a516\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"bfc150d8-274f-4e7e-8460-1c48f0d20988\",\"type\":\"BasicTicker\"}},\"id\":\"e3c0efb0-cfd2-424f-b12d-ac5122ef8bcf\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"01264c65-1fc1-4c3d-bcd1-d68a67694653\",\"type\":\"BasicTicker\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"ce0c0859-24b9-4398-8931-ff3b8e71a516\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"bfc150d8-274f-4e7e-8460-1c48f0d20988\",\"type\":\"BasicTicker\"}},\"id\":\"65014291-4b1b-4f2c-bfb5-92493a06eee0\",\"type\":\"Grid\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x0\",\"y0\",\"x1\",\"y1\"],\"data\":{\"x0\":[0,0,0,0],\"x1\":[0.13,0.8,0.99,0.71],\"y0\":[0,0,0,0],\"y1\":[0.99,0.6,0.13,0.71]}},\"id\":\"9c26413c-8964-4f6e-a9b1-3423d1422b04\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"source\":{\"id\":\"9c26413c-8964-4f6e-a9b1-3423d1422b04\",\"type\":\"ColumnDataSource\"}},\"id\":\"16921fa0-c7d6-4f53-8eff-0ae2c7a5e90a\",\"type\":\"CDSView\"},{\"attributes\":{\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"line_width\":{\"value\":3},\"x0\":{\"field\":\"x0\"},\"x1\":{\"field\":\"x1\"},\"y0\":{\"field\":\"y0\"},\"y1\":{\"field\":\"y1\"}},\"id\":\"71f096e9-90aa-4dd2-b752-319f57284ce4\",\"type\":\"Segment\"},{\"attributes\":{\"line_color\":{\"value\":\"#F4A582\"},\"line_width\":{\"value\":3},\"x0\":{\"field\":\"x0\"},\"x1\":{\"field\":\"x1\"},\"y0\":{\"field\":\"y0\"},\"y1\":{\"field\":\"y1\"}},\"id\":\"ced651a5-c802-4e20-8b2b-5909d9a97640\",\"type\":\"Segment\"}],\"root_ids\":[\"ce0c0859-24b9-4398-8931-ff3b8e71a516\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.14\"}};\n",
       "  var render_items = [{\"docid\":\"703118b8-5fe3-4d3d-b639-775c324461cb\",\"elementid\":\"20334130-8f06-4509-a281-e6c811ac8470\",\"modelid\":\"ce0c0859-24b9-4398-8931-ff3b8e71a516\"}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "ce0c0859-24b9-4398-8931-ff3b8e71a516"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook()\n",
    "p = figure(width=400, height=400)\n",
    "p.segment(x0=[0,0,0,0], y0=[0,0,0,0], x1=[d1[0], d2[0], d3[0], q[0]],\n",
    "          y1=[d1[1], d2[1], d3[1], q[1]], color=\"#F4A582\", line_width=3)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which of these is closest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul([d1,d2,d3],q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is going on here? Recall how the *dot/inner product* works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $q \\cdot d = q_1d_1 + \\ldots + q_nd_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is related to the *smallest angle between the two vectors* through the equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\cos(q,d) = \\frac{q \\cdot d}{\\|q\\|\\|d\\|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When the vectors are already normalized, as is the case here, we need only compute the dot products and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is equivalent to comparing *distances from the query vector* in the sense that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\cos(q,d_1) > \\cos(q,d_2) \\iff \\|q-d_1\\| < \\|q-d_2\\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An accompanying metric sometimes used is the *collection frequency*, $CF$ - the total number of occurrences of a term in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are many variants of this metric, and you can find all of them in use. The so-called *augmented* TF-IDF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $0.5 + \\frac{0.5\\times tf_{t,d}}{\\max_t tf_{t,d}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Distribution Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General idea: let's try to characterize how informative a word is. From *Manning & Schütze*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One could cast the problem as one of distinguishing content words from non-content (or function) words, but most models have a graded notion of how informative a word is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Zipf's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $f \\cdot r = k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Poisson Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $p(k;\\lambda_i) = e^{-\\lambda_i}\\frac{\\lambda_i^k}{k!}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, $\\lambda_i$ is the average number of occurrences of term $w_i$ per document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notebook-images/poiss.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The poisson is not a great fit for term distributions. But..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can exploit term distribution models in information retrieval by using the parameters of the model fit for a particular term as indicators of relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another model, the *K mixture* model of Katz, has larger $\\beta$ parameter for content words than for non-content words.\n",
    "\n",
    "### (see *Manning & Schütze* for more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAUTION: do not confuse collocations with $n$-grams. It is not actually confusing, but the definition of *collocation* is just not terribly precise. From *Manning & Schütze*,\n",
    "\n",
    ">A collocation is an expression consisting of two or more words that correspond to some conventional way of saying things.\n",
    "\n",
    "### Examples include \"stiff breeze\", \"broad daylight\" and \"international best practice\".\n",
    "\n",
    "### Collocations are not necessarily idioms, but idioms are (the extreme of) collocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look for some. How about `reuters` this time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "# For bigrams\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# For trigrams\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "#\n",
    "finder = BigramCollocationFinder.from_words(reuters.words())\n",
    "finder.nbest(bigram_measures.pmi, 100) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that in computing 'collocations', we are forced to take a stab at it useful some mathematical simplification of the general idea - here we used *pointwise mutual information*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This compares the actual joint probability of two events with the products of their probabilities. The higher it is, the stronger their association."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that a lot of the pairs look pretty useless. Some are not even recognizable parts of natural language, and many appear to be the names of particular humans. We can do some filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_freq_filter(3)\n",
    "finder.nbest(bigram_measures.pmi, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still looking mostly like names, but there is some other good stuff now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You all have heard of synonyms. But `WordNet` has a big collection of synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `WordNet` is a semantically oriented dictionary of English, similar to a traditional thesaurus but with richer structure. It is the most popular such thing among NLP types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It contains hundreds of thousands of words and *synonym sets*. For example, take sentences $A$ and $B$\n",
    "\n",
    ">$A)$ Benz is credited with the invention of the $\\color{red}{\\text{motorcar}}$.\n",
    "\n",
    ">$B)$ Benz is credited with the invention of the $\\color{red}{\\text{automobile}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sentences $A$ and $B$ have pretty much the same meaning. Let's see what `WordNet` has to offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "motorcar_syn = wn.synsets('motorcar')[0]\n",
    "motorcar_syn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a collection of synonymous words or *lemmas*. Remember *lemmatization*? We looked at some lemmatizers before, and we said that lemmatization is a harder task than stemming (and makes use of dictionaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorcar_syn.lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each word of a synset can have several meanings, e.g. \"car\" can signify \"train carriage\", \"gondola\" or \"elevator car\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorcar_syn.definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `WordNet` gives us examples, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorcar_syn.examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The word \"motorcar\" is pretty precise, so let's look at just \"car\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_syns = wn.synsets('car')\n",
    "for synset in car_syns:\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some lexical relationships hold between lemmas, e.g. *antonymy*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wn.lemma('supply.n.02.supply').antonyms(),'\\n',\n",
    "wn.lemma('rush.v.01.rush').antonyms(),'\\n',\n",
    "wn.lemma('horizontal.a.01.horizontal').antonyms(),'\\n',\n",
    "wn.lemma('staccato.r.01.staccato').antonyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $n$-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We used *bigrams* above and we touched on $n$-grams before. The `nltk` package implements $n$-grams in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have the n-grams tools in here by name\n",
    "from nltk import bigrams, trigrams, ngrams\n",
    "\n",
    "# A tweet\n",
    "t = '''#qcpoli enjoyed a hearty laugh today with #plq debate audience for @jflisee \n",
    "        #notrehome tune was that the intended reaction?'''\n",
    "\n",
    "# A tweet tokenizer\n",
    "tt = nltk.TweetTokenizer(t)\n",
    "\n",
    "# Tokens from our tweet\n",
    "tokens = tt.tokenize(t)\n",
    "\n",
    "# N-Grams from our tweet\n",
    "for t in bigrams(tokens): \n",
    "    print(t)\n",
    "for t in trigrams(tokens): \n",
    "    print(t)\n",
    "for t in ngrams(tokens, 4): \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall how we modeled the documents before, recording the presence or absence of certain (frequent) words. Those were *unigrams*. We can do the same thing with these $n$-grams as features! It just takes some more computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But $n$-grams actually lead us to a more powerful embedding of words into vector space. More on that Thursday."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
